{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./repositories/pytorch-image-models')\n",
    "sys.path.append('./repositories/segmentation_models.pytorch')\n",
    "\n",
    "import math\n",
    "import collections\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from denk_baseline.lightning_models import ClassificationBase\n",
    "from denk_baseline.datamodules import DataModule\n",
    "from run import preprocess_config, parse_loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('./rsna_config.yaml')\n",
    "config = preprocess_config(config)\n",
    "\n",
    "# config['common']['gpus'] = [1]\n",
    "# config['common']['batch_size'] = 8\n",
    "# config['common']['exp_name'] = 'fold-3-PVTv2-Unet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(config['common']['seed'], workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixInterpoltion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mixing = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        device = x.device\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.mixing*F.interpolate(x, size, mode='bilinear', align_corners=False) + (1-self.mixing )*F.interpolate(x, size, mode='nearest')\n",
    "        x = x.to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bytedance/Next-ViT\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "\n",
    "NORM_EPS = 1e-5\n",
    "\n",
    "def merge_pre_bn(module, pre_bn_1, pre_bn_2=None):\n",
    "    \"\"\" Merge pre BN to reduce inference runtime.\n",
    "    \"\"\"\n",
    "    weight = module.weight.data\n",
    "    if module.bias is None:\n",
    "        zeros = torch.zeros(module.out_channels, device=weight.device).type(weight.type())\n",
    "        module.bias = nn.Parameter(zeros)\n",
    "    bias = module.bias.data\n",
    "    if pre_bn_2 is None:\n",
    "        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
    "        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n",
    "\n",
    "        scale_invstd = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n",
    "        extra_weight = scale_invstd * pre_bn_1.weight\n",
    "        extra_bias = pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd\n",
    "    else:\n",
    "        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
    "        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n",
    "\n",
    "        assert pre_bn_2.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
    "        assert pre_bn_2.affine is True, \"Unsupport bn_module.affine is False\"\n",
    "\n",
    "        scale_invstd_1 = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n",
    "        scale_invstd_2 = pre_bn_2.running_var.add(pre_bn_2.eps).pow(-0.5)\n",
    "\n",
    "        extra_weight = scale_invstd_1 * pre_bn_1.weight * scale_invstd_2 * pre_bn_2.weight\n",
    "        extra_bias = scale_invstd_2 * pre_bn_2.weight *(pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd_1 - pre_bn_2.running_mean) + pre_bn_2.bias\n",
    "\n",
    "    if isinstance(module, nn.Linear):\n",
    "        extra_bias = weight @ extra_bias\n",
    "        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        assert weight.shape[2] == 1 and weight.shape[3] == 1\n",
    "        weight = weight.reshape(weight.shape[0], weight.shape[1])\n",
    "        extra_bias = weight @ extra_bias\n",
    "        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n",
    "        weight = weight.reshape(weight.shape[0], weight.shape[1], 1, 1)\n",
    "    bias.add_(extra_bias)\n",
    "\n",
    "    module.weight.data = weight\n",
    "    module.bias.data = bias\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            groups=1):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                              padding=1, groups=groups, bias=False)\n",
    "        self.norm = nn.BatchNorm2d(out_channels, eps=NORM_EPS)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 stride=1):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
    "        if stride == 2:\n",
    "            self.avgpool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        elif in_channels != out_channels:\n",
    "            self.avgpool = nn.Identity()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "            self.norm = norm_layer(out_channels)\n",
    "        else:\n",
    "            self.avgpool = nn.Identity()\n",
    "            self.conv = nn.Identity()\n",
    "            self.norm = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(self.conv(self.avgpool(x)))\n",
    "\n",
    "\n",
    "class MHCA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Convolutional Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels, head_dim):\n",
    "        super(MHCA, self).__init__()\n",
    "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
    "        self.group_conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                                       padding=1, groups=out_channels // head_dim, bias=False)\n",
    "        self.norm = norm_layer(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.projection = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.group_conv3x3(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, out_features=None, mlp_ratio=None, drop=0., bias=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = _make_divisible(in_features * mlp_ratio, 32)\n",
    "        self.conv1 = nn.Conv2d(in_features, hidden_dim, kernel_size=1, bias=bias)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, out_features, kernel_size=1, bias=bias)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def merge_bn(self, pre_norm):\n",
    "        merge_pre_bn(self.conv1, pre_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NCB(nn.Module):\n",
    "    \"\"\"\n",
    "    Next Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n",
    "                 drop=0, head_dim=32, mlp_ratio=3):\n",
    "        super(NCB, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
    "        assert out_channels % head_dim == 0\n",
    "\n",
    "        self.patch_embed = PatchEmbed(in_channels, out_channels, stride)\n",
    "        self.mhca = MHCA(out_channels, head_dim)\n",
    "        self.attention_path_dropout = DropPath(path_dropout)\n",
    "\n",
    "        self.norm = norm_layer(out_channels)\n",
    "        self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop, bias=True)\n",
    "        self.mlp_path_dropout = DropPath(path_dropout)\n",
    "        self.is_bn_merged = False\n",
    "\n",
    "    def merge_bn(self):\n",
    "        if not self.is_bn_merged:\n",
    "            self.mlp.merge_bn(self.norm)\n",
    "            self.is_bn_merged = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.attention_path_dropout(self.mhca(x))\n",
    "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
    "            out = self.norm(x)\n",
    "        else:\n",
    "            out = x\n",
    "        x = x + self.mlp_path_dropout(self.mlp(out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class E_MHSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Multi-Head Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n",
    "                 attn_drop=0, proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.num_heads = self.dim // head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(self.dim, self.out_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        self.N_ratio = sr_ratio ** 2\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n",
    "            self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n",
    "        self.is_bn_merged = False\n",
    "\n",
    "    def merge_bn(self, pre_bn):\n",
    "        merge_pre_bn(self.q, pre_bn)\n",
    "        if self.sr_ratio > 1:\n",
    "            merge_pre_bn(self.k, pre_bn, self.norm)\n",
    "            merge_pre_bn(self.v, pre_bn, self.norm)\n",
    "        else:\n",
    "            merge_pre_bn(self.k, pre_bn)\n",
    "            merge_pre_bn(self.v, pre_bn)\n",
    "        self.is_bn_merged = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x)\n",
    "        q = q.reshape(B, N, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.transpose(1, 2)\n",
    "            x_ = self.sr(x_)\n",
    "            if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
    "                x_ = self.norm(x_)\n",
    "            x_ = x_.transpose(1, 2)\n",
    "            k = self.k(x_)\n",
    "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
    "            v = self.v(x_)\n",
    "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            k = self.k(x)\n",
    "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
    "            v = self.v(x)\n",
    "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
    "        attn = (q @ k) * self.scale\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NTB(nn.Module):\n",
    "    \"\"\"\n",
    "    Next Transformer Block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, path_dropout, stride=1, sr_ratio=1,\n",
    "            mlp_ratio=2, head_dim=32, mix_block_ratio=0.75, attn_drop=0, drop=0,\n",
    "    ):\n",
    "        super(NTB, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.mix_block_ratio = mix_block_ratio\n",
    "        norm_func = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
    "\n",
    "        self.mhsa_out_channels = _make_divisible(int(out_channels * mix_block_ratio), 32)\n",
    "        self.mhca_out_channels = out_channels - self.mhsa_out_channels\n",
    "\n",
    "        self.patch_embed = PatchEmbed(in_channels, self.mhsa_out_channels, stride)\n",
    "        self.norm1 = norm_func(self.mhsa_out_channels)\n",
    "        self.e_mhsa = E_MHSA(self.mhsa_out_channels, head_dim=head_dim, sr_ratio=sr_ratio,\n",
    "                             attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.mhsa_path_dropout = DropPath(path_dropout * mix_block_ratio)\n",
    "\n",
    "        self.projection = PatchEmbed(self.mhsa_out_channels, self.mhca_out_channels, stride=1)\n",
    "        self.mhca = MHCA(self.mhca_out_channels, head_dim=head_dim)\n",
    "        self.mhca_path_dropout = DropPath(path_dropout * (1 - mix_block_ratio))\n",
    "\n",
    "        self.norm2 = norm_func(out_channels)\n",
    "        self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop)\n",
    "        self.mlp_path_dropout = DropPath(path_dropout)\n",
    "\n",
    "        self.is_bn_merged = False\n",
    "\n",
    "    def merge_bn(self):\n",
    "        if not self.is_bn_merged:\n",
    "            self.e_mhsa.merge_bn(self.norm1)\n",
    "            self.mlp.merge_bn(self.norm2)\n",
    "            self.is_bn_merged = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B, C, H, W = x.shape\n",
    "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
    "            out = self.norm1(x)\n",
    "        else:\n",
    "            out = x\n",
    "        out = rearrange(out, \"b c h w -> b (h w) c\")  # b n c\n",
    "        out = self.mhsa_path_dropout(self.e_mhsa(out))\n",
    "        x = x + rearrange(out, \"b (h w) c -> b c h w\", h=H)\n",
    "\n",
    "        out = self.projection(x)\n",
    "        out = out + self.mhca_path_dropout(self.mhca(out))\n",
    "        x = torch.cat([x, out], dim=1)\n",
    "\n",
    "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
    "            out = self.norm2(x)\n",
    "        else:\n",
    "            out = x\n",
    "        x = x + self.mlp_path_dropout(self.mlp(out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class NextViT(nn.Module):\n",
    "    def __init__(self, stem_chs, depths, path_dropout, attn_drop=0, drop=0, num_classes=1000,\n",
    "                 strides=[1, 2, 2, 2], sr_ratios=[8, 4, 2, 1], head_dim=32, mix_block_ratio=0.75):\n",
    "        super(NextViT, self).__init__()\n",
    "        self.depths = [depths[0], depths[1], 5*(depths[2] // 5), depths[3]]\n",
    "        self.stage_out_channels = [[96] * (depths[0]),\n",
    "                                   [192] * (depths[1] - 1) + [256],\n",
    "                                   [384, 384, 384, 384, 512] * (depths[2] // 5),\n",
    "                                   [768] * (depths[3] - 1) + [1024]]\n",
    "\n",
    "        self.stage_block_types = [[NCB] * depths[0],\n",
    "                                  [NCB] * (depths[1] - 1) + [NTB],\n",
    "                                  [NCB, NCB, NCB, NCB, NTB] * (depths[2] // 5),\n",
    "                                  [NCB] * (depths[3] - 1) + [NTB]]\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBNReLU(3, stem_chs[0], kernel_size=3, stride=2),\n",
    "            ConvBNReLU(stem_chs[0], stem_chs[1], kernel_size=3, stride=1),\n",
    "            ConvBNReLU(stem_chs[1], stem_chs[2], kernel_size=3, stride=1),\n",
    "            ConvBNReLU(stem_chs[2], stem_chs[2], kernel_size=3, stride=2),\n",
    "        )\n",
    "        input_channel = stem_chs[-1]\n",
    "        features = []\n",
    "        aux_ln = []\n",
    "        aux_bn = []\n",
    "        aux_in = []\n",
    "        idx = 0\n",
    "        dpr = [x.item() for x in torch.linspace(0, path_dropout, sum(depths))]  # stochastic depth decay rule\n",
    "        for stage_id in range(len(depths)):\n",
    "            numrepeat = depths[stage_id]\n",
    "            output_channels = self.stage_out_channels[stage_id]\n",
    "            block_types = self.stage_block_types[stage_id]\n",
    "            aux_ln.append(\n",
    "                    nn.Sequential(\n",
    "                    nn.Linear(output_channels[-1], num_classes),\n",
    "                )\n",
    "            )\n",
    "            aux_bn.append(\n",
    "                    nn.Sequential(\n",
    "                    nn.BatchNorm2d(output_channels[-1], eps=NORM_EPS),\n",
    "                )\n",
    "            )\n",
    "            aux_in.append(MixInterpoltion())\n",
    "            \n",
    "            for block_id in range(numrepeat):\n",
    "                if strides[stage_id] == 2 and block_id == 0:\n",
    "                    stride = 2\n",
    "                else:\n",
    "                    stride = 1\n",
    "                output_channel = output_channels[block_id]\n",
    "                block_type = block_types[block_id]\n",
    "                if block_type is NCB:\n",
    "                    layer = NCB(input_channel, output_channel, stride=stride, path_dropout=dpr[idx + block_id],\n",
    "                                drop=drop, head_dim=head_dim)\n",
    "                    features.append(layer)\n",
    "                elif block_type is NTB:\n",
    "                    layer = NTB(input_channel, output_channel, path_dropout=dpr[idx + block_id], stride=stride,\n",
    "                                sr_ratio=sr_ratios[stage_id], head_dim=head_dim, mix_block_ratio=mix_block_ratio,\n",
    "                                attn_drop=attn_drop, drop=drop)\n",
    "                    features.append(layer)\n",
    "                input_channel = output_channel\n",
    "            idx += numrepeat\n",
    "        self.features = nn.Sequential(*features)\n",
    "        \n",
    "        self.aux_bn0 = aux_bn[0]\n",
    "        self.aux_ln0 = aux_ln[0]\n",
    "        \n",
    "        self.aux_bn1 = aux_bn[1]\n",
    "        self.aux_ln1 = aux_ln[1]\n",
    "        self.aux_in1 = aux_in[1]\n",
    "        \n",
    "        self.aux_bn2 = aux_bn[2]\n",
    "        self.aux_ln2 = aux_ln[2]\n",
    "        self.aux_in2 = aux_in[2]\n",
    "        \n",
    "        self.aux_bn3 = aux_bn[3]\n",
    "        self.aux_ln3 = aux_ln[3]\n",
    "        self.aux_in3 = aux_in[3]\n",
    "        \n",
    "        self.norm = nn.BatchNorm2d(sum([x[-1] for x in self.stage_out_channels]), eps=NORM_EPS)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(sum([x[-1] for x in self.stage_out_channels]), num_classes),\n",
    "        )\n",
    "\n",
    "        self.stage_out_idx = [sum(depths[:idx + 1]) - 1 for idx in range(len(depths))]\n",
    "        print('initialize_weights...')\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def merge_bn(self):\n",
    "        self.eval()\n",
    "        for idx, module in self.named_modules():\n",
    "            if isinstance(module, NCB) or isinstance(module, NTB):\n",
    "                module.merge_bn()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['image']\n",
    "        x = self.stem(x)\n",
    "        out = {\n",
    "            'loss': 0.0,\n",
    "            'predict': None,\n",
    "        }\n",
    "        t_feats = []\n",
    "        f_index = 0\n",
    "        for b_index, d_num in enumerate(self.depths):\n",
    "            for i in range(d_num):\n",
    "                x = self.features[f_index](x)\n",
    "                f_index+=1\n",
    "            t_feats.append(x)\n",
    "            \n",
    "            if b_index == 0:\n",
    "                aux_x = self.aux_bn0(x)\n",
    "                aux_x = self.avgpool(aux_x)\n",
    "                aux_x = torch.flatten(aux_x, 1)\n",
    "                aux_x = self.aux_ln0(aux_x)\n",
    "                aux_x = torch.nan_to_num(aux_x, neginf=0.0, posinf=1.0)\n",
    "                out['loss'] += 0.05 * get_loss(aux_x, batch['label'])\n",
    "            elif b_index == 1:\n",
    "                aux_x = self.aux_bn1(x)\n",
    "                aux_x = self.avgpool(aux_x)\n",
    "                aux_x = torch.flatten(aux_x, 1)\n",
    "                aux_x = self.aux_ln1(aux_x)\n",
    "                aux_x = torch.nan_to_num(aux_x, neginf=0.0, posinf=1.0)\n",
    "                out['loss'] += 0.10 * get_loss(aux_x, batch['label'])\n",
    "            elif b_index == 2:\n",
    "                aux_x = self.aux_bn2(x)\n",
    "                aux_x = self.avgpool(aux_x)\n",
    "                aux_x = torch.flatten(aux_x, 1)\n",
    "                aux_x = self.aux_ln2(aux_x)\n",
    "                aux_x = torch.nan_to_num(aux_x, neginf=0.0, posinf=1.0)\n",
    "                out['loss'] += 0.15 * get_loss(aux_x, batch['label'])\n",
    "            elif b_index == 3:\n",
    "                aux_x = self.aux_bn3(x)\n",
    "                aux_x = self.avgpool(aux_x)\n",
    "                aux_x = torch.flatten(aux_x, 1)\n",
    "                aux_x = self.aux_ln3(aux_x)\n",
    "                aux_x = torch.nan_to_num(aux_x, neginf=0.0, posinf=1.0)\n",
    "                out['loss'] += 0.20 * get_loss(aux_x, batch['label'])\n",
    "#             print(aux_x, batch['label'])\n",
    "#             print(b_index, out['loss'])\n",
    "        \n",
    "        h, w = t_feats[0].shape[-2:]\n",
    "        \n",
    "        t_feats[1] = self.aux_in1(t_feats[1], (h, w))\n",
    "        t_feats[2] = self.aux_in1(t_feats[2], (h, w))\n",
    "        t_feats[3] = self.aux_in1(t_feats[3], (h, w))\n",
    "        \n",
    "        x = torch.cat(t_feats, dim=1)\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.proj_head(x)\n",
    "        x = torch.nan_to_num(x, neginf=0.0, posinf=1.0)\n",
    "        out['loss'] += get_loss(x, batch['label'])\n",
    "        out['predict'] = x\n",
    "        return out\n",
    "\n",
    "def bce_criterion(pr, gt):\n",
    "    loss = F.binary_cross_entropy_with_logits(pr, gt)\n",
    "    return loss\n",
    "\n",
    "def get_loss(pr, gt):\n",
    "    return bce_criterion(pr, gt)\n",
    "\n",
    "\n",
    "# b = {\n",
    "#     'image': torch.rand(4, 3, 768, 768),\n",
    "#     'label': torch.rand(4, 1),\n",
    "# }\n",
    "\n",
    "# net = NextViT(stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, num_classes=1)\n",
    "# net(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationBinaryModel(ClassificationBase):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)        \n",
    "        self.model = NextViT(stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, num_classes=1)\n",
    "        ckpt = torch.load('./pretrain_weights/nextvit_bm.pt', map_location='cpu')\n",
    "        self.model.load_state_dict(ckpt['state_dict'], strict=False)\n",
    "#         print(self.kornia_augs)\n",
    "        \n",
    "    def _common_step(self, batch, batch_idx, stage):\n",
    "        gt_img, gt_label = batch['image'], batch['label'].float().unsqueeze(1)\n",
    "        if self.kornia_augs:\n",
    "            gt_img = self.kornia_augs(gt_img)\n",
    "        n_batch = {\n",
    "            'image': gt_img.contiguous(),\n",
    "            'label': gt_label,\n",
    "        }\n",
    "        predicts = self.model(n_batch)\n",
    "        pr_label = predicts['predict']\n",
    "        loss = predicts['loss']\n",
    "        \n",
    "        self.log(f'total_loss_{stage}', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.metric_values[stage]['pr'].append(pr_label.cpu().detach().squeeze())\n",
    "        self.metric_values[stage]['gt'].append(gt_label.cpu().detach().squeeze().long())\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize_weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize_weights...\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationBinaryModel(config)\n",
    "\n",
    "# ckpt = torch.load('./output/hubmap-2022/fold-3-PVTv2-Coa-seed-17/last.ckpt', map_location='cpu')\n",
    "# model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = parse_loggers(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/user/Projects/denk_baseline/output/rsna-2022/nextvit-b-256-0 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | NextViT    | 43.8 M\n",
      "1 | kornia_augs | KorniaAugs | 0     \n",
      "-------------------------------------------\n",
      "43.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.8 M    Total params\n",
      "175.213   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  13%|█████████████▋                                                                                           | 197/1513 [00:54<06:04,  3.61it/s, loss=1.39, v_num=56-0]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['common']['max_epochs'], \n",
    "    gpus=config['trainer']['params']['gpus'],\n",
    "    logger=[v for _, v in loggers.items()],\n",
    "    precision='bf16',\n",
    ")\n",
    "trainer.fit(model, datamodule)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
