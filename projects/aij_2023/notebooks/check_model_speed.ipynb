{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85157bf-5399-4113-ba92-06c4cdd68a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/pytorch-image-models') \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import openvino\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from models.swiftformer import SwiftFormer\n",
    "from models.swiftformer3d import SwiftFormer3D\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "from models.mobilenetv2 import MobileNetV2\n",
    "from models.mvit.model import MViTModel, CustomMvitModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48a48f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "torch_dtype = torch.float32\n",
    "numpy_dtype = np.float32\n",
    "\n",
    "n_skip = 5\n",
    "n_runs = 10\n",
    "print_step = 1\n",
    "batch_shape = [1, 3, 32, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e2439b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kwargs = {\n",
    "#     'num_classes': 1001,\n",
    "#     'frame_count': 32,\n",
    "#     'layers': [3, 3, 6, 4],\n",
    "#     'embed_dims': [48, 56, 112, 220],\n",
    "#     'temporal_indices': [4, 6],\n",
    "#     'conv3d_indices': [3, 5],\n",
    "#     'downsamples': [True, True, True, True],\n",
    "#     'vit_num': 1,\n",
    "# }\n",
    "# model = SwiftFormer(**model_kwargs).eval()\n",
    "\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'num_classes': 1001,\n",
    "#     'arch': 'mobilenet_v3_large', # 'mobilenet_v3_small'\n",
    "#     'frame_count': 32,\n",
    "#     'temporal_indices': [10, 12, 14],\n",
    "#     'conv3d_indices': [2, 4, 8],\n",
    "# }\n",
    "# model = MobileNetV3(**model_kwargs).eval()\n",
    "\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'num_classes': 1001,\n",
    "#     'sample_size': 224,\n",
    "#     'frame_count': 32,\n",
    "#     'temporal_indices': [1, 23, 4, 5, 6],\n",
    "#     'width_mult': 4,\n",
    "#     'with_two_heads': False,\n",
    "# }\n",
    "# model = MobileNetV2(**model_kwargs).eval()\n",
    "\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'num_classes': 1001,\n",
    "#     'head_channels': 384,\n",
    "#     'backbone_channels': 256,\n",
    "#     'arch': {\n",
    "#         'embed_dims': 96,\n",
    "#         'num_layers': 10,\n",
    "#         'num_heads': 1,\n",
    "#         'downscale_indices': [1, 3, 10]\n",
    "#     },\n",
    "#     # 'arch': 'small',\n",
    "#     # 'pretrained': './mvit32.2_small_state_dict.pt'\n",
    "# }\n",
    "\n",
    "# model = MViTModel(**model_kwargs).eval()\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'backbone': 'x2',\n",
    "#     'mvit_kwargs':{\n",
    "#       'num_classes': 1001,\n",
    "#       'backbone_channels': 64,\n",
    "#       'head_channels': 768,\n",
    "#       'ignore_layers': [],\n",
    "#       'arch': {\n",
    "#         'embed_dims': 96,\n",
    "#         'num_layers': 12,\n",
    "#         'num_heads': 1,\n",
    "#         'downscale_indices': [1, 3, 11],\n",
    "#       },\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# model = CustomMvitModel(**model_kwargs).eval()\n",
    "\n",
    "model_kwargs = {\n",
    "        'num_classes': 1001,\n",
    "        'frame_count': 32,\n",
    "        'arch': 'l3',  # XS, S, l1, l3\n",
    "        'temporal_indices': [1, 2, 3, 4, 5, 6],\n",
    "        'conv3d_indices': [],\n",
    "        'downsamples': [True, True, True, True],\n",
    "        'vit_num': 1,\n",
    "}\n",
    "model = SwiftFormer3D(**model_kwargs).eval().to(device=device)\n",
    "\n",
    "# model_kwargs = {\n",
    "#         'num_classes': 1001,\n",
    "#         'frame_count': 32,\n",
    "#         'arch': 'l3',  # XS, S, l1, l3\n",
    "#         'temporal_indices': [],\n",
    "#         'conv3d_indices': [],\n",
    "#         'downsamples': [True, True, True, True],\n",
    "#         'vit_num': 1,\n",
    "# }\n",
    "# model = SwiftFormer(**model_kwargs).eval().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "393526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_inputs = [np.random.randn(*batch_shape).astype(numpy_dtype) for _ in range(n_runs)]\n",
    "torch_inputs = [torch.from_numpy(x).to(device=device, dtype=torch_dtype) for x in numpy_inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf403",
   "metadata": {},
   "source": [
    "### FLOPS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d389d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.rand(*batch_shape).to(device=device, dtype=torch_dtype)\n",
    "# flops = FlopCountAnalysis(model, input_tensor)\n",
    "# total_flops = flops.total()\n",
    "# print(f'GFLOPS: {total_flops / 1e9:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320a823",
   "metadata": {},
   "source": [
    "### TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "33d89bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP ITERATION 5\n",
      "START TIME MESURING\n",
      "STEP: 1 | TIME 0.560 SEC.\n",
      "STEP: 2 | TIME 0.534 SEC.\n",
      "STEP: 3 | TIME 0.552 SEC.\n",
      "STEP: 4 | TIME 0.532 SEC.\n",
      "STEP: 5 | TIME 0.532 SEC.\n",
      "STEP: 6 | TIME 0.546 SEC.\n",
      "STEP: 7 | TIME 0.565 SEC.\n",
      "STEP: 8 | TIME 0.532 SEC.\n",
      "STEP: 9 | TIME 0.526 SEC.\n",
      "STEP: 10 | TIME 0.535 SEC.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for skip_num, input_data in enumerate(torch_inputs[:n_skip]):\n",
    "        print(f'SKIP ITERATION {skip_num + 1}', end='\\r')\n",
    "        features = model(input_data)\n",
    "\n",
    "    print('\\nSTART TIME MESURING')\n",
    "    torch_predicts = []\n",
    "    for n_run, input_data in enumerate(torch_inputs):\n",
    "        start_time = time.time()\n",
    "        torch_predict = model(input_data)\n",
    "        torch_predicts.append(torch_predict)\n",
    "        end_time = time.time()\n",
    "\n",
    "        time_delta = end_time - start_time\n",
    "        if (n_run + 1) % print_step == 0:\n",
    "            print(f'STEP: {n_run + 1} | TIME {time_delta:0.3f} SEC.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47940572",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54debbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bed2036a-795e-471d-b8f7-b619df6294f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(*batch_shape).to(device=device, dtype=torch_dtype)\n",
    "onnx_path = 'model.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    onnx_path, \n",
    "    input_names = ['input'], \n",
    "    output_names = ['output'], \n",
    "    dynamic_axes = {\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'output' : {0 : 'batch_size'},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8c631536",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "del onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "974e666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "15e7be14-a70a-48fa-b309-e08bda18bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP ITERATION 5\n",
      "START TIME MESURING\n",
      "STEP: 1 | TIME 0.648 SEC.\n",
      "STEP: 2 | TIME 0.656 SEC.\n",
      "STEP: 3 | TIME 0.649 SEC.\n",
      "STEP: 4 | TIME 0.657 SEC.\n",
      "STEP: 5 | TIME 0.690 SEC.\n",
      "STEP: 6 | TIME 0.659 SEC.\n",
      "STEP: 7 | TIME 0.657 SEC.\n",
      "STEP: 8 | TIME 0.685 SEC.\n",
      "STEP: 9 | TIME 0.691 SEC.\n",
      "STEP: 10 | TIME 0.678 SEC.\n"
     ]
    }
   ],
   "source": [
    "for skip_num, input_data in enumerate(numpy_inputs[:n_skip]):\n",
    "    print(f'SKIP ITERATION {skip_num + 1}', end='\\r')\n",
    "    outputs = ort_session.run(None, {'input': input_data})\n",
    "\n",
    "print('\\nSTART TIME MESURING')\n",
    "onnx_predicts = []\n",
    "for n_run, input_data in enumerate(numpy_inputs):\n",
    "    start_time = time.time()\n",
    "    onnx_predict = ort_session.run(None, {'input': input_data})\n",
    "    onnx_predicts.append(onnx_predict)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_delta = end_time - start_time\n",
    "    if (n_run + 1) % print_step == 0:\n",
    "        print(f'STEP: {n_run + 1} | TIME {time_delta:0.3f} SEC.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2023fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ort_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76658eee",
   "metadata": {},
   "source": [
    "### OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "421f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino import runtime, save_model, convert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "906f256f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ir_path = 'model.xml'\n",
    "ir_model = convert_model(onnx_path)\n",
    "save_model(ir_model, ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "620fd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = runtime.Core()\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "compiled_model_ir = core.compile_model(model=model_ir, device_name='CPU')\n",
    "\n",
    "output_layer_ir = compiled_model_ir.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "02514082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mo --input_model model.onnx --output_dir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2cf04669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_path = './model/model.xml'\n",
    "# bin_path = './model/model.bin'\n",
    "\n",
    "# core = runtime.Core()\n",
    "# model_ir = core.read_model(model=xml_path, weights=bin_path)\n",
    "# compiled_model_ir = core.compile_model(model=model_ir)\n",
    "\n",
    "# output_layer_ir = compiled_model_ir.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "85a494ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP ITERATION 5\n",
      "START TIME MESURING\n",
      "STEP: 1 | TIME 0.221 SEC.\n",
      "STEP: 2 | TIME 0.221 SEC.\n",
      "STEP: 3 | TIME 0.233 SEC.\n",
      "STEP: 4 | TIME 0.221 SEC.\n",
      "STEP: 5 | TIME 0.219 SEC.\n",
      "STEP: 6 | TIME 0.227 SEC.\n",
      "STEP: 7 | TIME 0.228 SEC.\n",
      "STEP: 8 | TIME 0.213 SEC.\n",
      "STEP: 9 | TIME 0.221 SEC.\n",
      "STEP: 10 | TIME 0.216 SEC.\n"
     ]
    }
   ],
   "source": [
    "for skip_num, input_data in enumerate(numpy_inputs[:n_skip]):\n",
    "    print(f'SKIP ITERATION {skip_num + 1}', end='\\r')\n",
    "    res_ir = compiled_model_ir(input_data)[output_layer_ir]\n",
    "\n",
    "print('\\nSTART TIME MESURING')\n",
    "openvino_predicts = []\n",
    "for n_run, input_data in enumerate(numpy_inputs):\n",
    "    start_time = time.time()\n",
    "    openvino_predict = compiled_model_ir(input_data)[output_layer_ir]\n",
    "    openvino_predicts.append(openvino_predict)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_delta = end_time - start_time\n",
    "    if (n_run + 1) % print_step == 0:\n",
    "        print(f'STEP: {n_run + 1} | TIME {time_delta:0.3f} SEC.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd66fe",
   "metadata": {},
   "source": [
    "### CHECK PREDICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "386a008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0.6)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_compares = []\n",
    "\n",
    "for torch_values, onnx_values in zip(torch_predicts, onnx_predicts):\n",
    "#     onnx_is_close = np.allclose(torch_values.numpy(), onnx_values, rtol=1e-01, atol=1e-01)\n",
    "#     onnx_compares.append(onnx_is_close)\n",
    "    torch_value = torch_values.argmax().item()\n",
    "    onnx_value = np.array(onnx_predicts[0][0][0]).argmax()\n",
    "    onnx_compares.append(torch_value == onnx_value)\n",
    "\n",
    "all(onnx_compares), sum(onnx_compares) / len(onnx_compares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5797a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0.6)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openvino_compares = []\n",
    "\n",
    "for torch_values, openvino_values in zip(torch_predicts, openvino_predicts):\n",
    "#     openvino_is_close = np.allclose(torch_values.numpy(), openvino_values, rtol=1e-01, atol=1e-01)\n",
    "#     openvino_compares.append(openvino_is_close)\n",
    "    torch_value = torch_values.argmax().item()\n",
    "    openvino_value = np.array(onnx_predicts[0][0][0]).argmax()\n",
    "    openvino_compares.append(torch_value == onnx_value)\n",
    "    \n",
    "all(openvino_compares), sum(openvino_compares) / len(openvino_compares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0c253d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(torch_values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df0d0378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(onnx_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "945a7fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(openvino_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4504790c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAAhCAYAAADqIMMzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACi0lEQVR4nO3csWoiURTG8W9uAsHKNpPCJnUKCx/AYiCPE5A0aeIrWPkaFjb6HILgGwgWgkEJmJktQnZhDcsmM/HcOfP/lZPm4Bfud64xJkVRFAIAAGaC9QAAADQdZQwAgDHKGAAAY5QxAADGKGMAAIxRxgAAGKOMAQAwRhkDAGDs0noAK0VR6HA46LPvPLm6utLlZWNfmrMiB5zD8XjU6+vryfMkSdRqtZQkicFUIJc/kqZ+A9fb25v6/b42m83Jz0ajkbIsM5iqeTzlwGIRr8lkoqenp5PnNzc3ms/nCoE3CS14yKWqheLLp4OnA2e1Wmm9Xp883+12BtN8DTnEJ89z3d/fu1gsPni5uWy3Wy2Xy5Pn+/3eYJryyCUe0+n0nwvFj5WxxwNHkkIIv1+0Ovwik0OcvCwWH6o6aKyFEHRxcSHpfZHN89x4onLIJR5VLRTfuj55O3AkaTwe6/b2VpJ0d3dnPM3/IYe41X2xkHzcXCQpyzLNZjNJ0mKx0MPDg+1AJZFLPKpaKEq/l+nhwJGkXq+nbrdrPca3kUN8PCwWHm4ukpSmqdI0lfT+Z5y6I5d4VLVQlC5jDweOB+QQHw+LhYebi0fkEo+qForSZezhwPGAHPATPNxcPCIXf+L/3DgAAM5RxgAAGKvPP6NWLEkSPT4+6uXlRZJ0fX1tPFEzkQPOrdPpaDgcSpLa7XatP/DoSdNzaWwZhxA0GAysx2g8TzmwWNRDp9PR8/Oz9Rj4i4dcyiwUXy5jDpw4kEN8PC0Wn2n6zSVW5BKPMgtFY7+bGgCAWPABLgAAjFHGAAAYo4wBADBGGQMAYIwyBgDAGGUMAIAxyhgAAGOUMQAAxihjAACMUcYAABj7BXpodfswL13zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from denku import show_image, show_images\n",
    "\n",
    "\n",
    "def crop_lt(x, crop_h, crop_w):\n",
    "    return x[:, :, :, 0:crop_h, 0:crop_w]\n",
    "\n",
    "def crop_lb(x, crop_h, crop_w):\n",
    "    return x[:, :, :, -crop_h:, 0:crop_w]\n",
    "\n",
    "def crop_rt(x, crop_h, crop_w):\n",
    "    return x[:, :, :, 0:crop_h, -crop_w:]\n",
    "\n",
    "def crop_rb(x, crop_h, crop_w):\n",
    "    return x[:, :, :, -crop_h:, -crop_w:]\n",
    "\n",
    "def center_crop(x, crop_h, crop_w):\n",
    "    center_h = x.shape[3] // 2\n",
    "    center_w = x.shape[4] // 2\n",
    "    half_crop_h = crop_h // 2\n",
    "    half_crop_w = crop_w // 2\n",
    "\n",
    "    y_min = center_h - half_crop_h\n",
    "    y_max = center_h + half_crop_h + crop_h % 2\n",
    "    x_min = center_w - half_crop_w\n",
    "    x_max = center_w + half_crop_w + crop_w % 2\n",
    "\n",
    "    return x[:, :, :, y_min:y_max, x_min:x_max]\n",
    "\n",
    "def five_crops(x, crop_h, crop_w):\n",
    "    return torch.cat([f(x, crop_h, crop_w) for f in [crop_lt, crop_lb, crop_rt, crop_rb, center_crop]])\n",
    "\n",
    "def ten_crops(x, crop_h, crop_w):\n",
    "    five_crop = five_crops(x, crop_h, crop_w)\n",
    "    ten_crops = torch.cat([five_crop, five_crop.flip(dims=(4,))])\n",
    "    return ten_crops\n",
    "    \n",
    "batch = torch.ones(1, 3, 2, 256, 256) * 255\n",
    "batch[:, :, :, 50: 200, 50: 100] = 0\n",
    "batch[:, :, :, 50: 100, 100: 150] = 0\n",
    "\n",
    "crops = ten_crops(batch, 224, 224)\n",
    "# for crop in crops:\n",
    "#     print(crop.shape)\n",
    "\n",
    "show_images([\n",
    "    batch[0, :, 0, :, :].permute(1, 2, 0).numpy().astype(np.uint8),\n",
    "])\n",
    "show_images([\n",
    "    crop[:, 0, :, :].permute(1, 2, 0).numpy().astype(np.uint8) for crop in crops\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1efdd561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28da9693-e1ec-4e20-81a9-0a0e916f7a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2072, 0.1240, 0.0761, 0.1932, 0.1204, 0.0766, 0.1199, 0.0825])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 8).sum(dim=0).softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45f6e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'model.conv'\n",
    "name[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69c26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df0502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d10d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad8f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
